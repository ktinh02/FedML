common_args:
 training_type: "cross_silo"
 random_seed: 0
 scenario: "horizontal"
 using_mlops: false
 config_version: release
 name: "fednlp_experiment"
 project: "runs/train"
 exist_ok: false


data_args:
 dataset: "BBC_news"
 data_file_path: "/home/ktinh/Documents/GitHub/FedML/python/examples/federate/prebuilt_jobs/fednlp/text_classification/data/BBC_news.h5"
 partition_file_path: "/home/ktinh/Documents/GitHub/FedML/python/examples/federate/prebuilt_jobs/fednlp/text_classification/data/BBC_news_partition.h5"
 partition_method: "uniform"
 reprocess_input_data: false
 global_model_file_path: "/home/ktinh/fednlp_models/global_model.pt"
 model_file_cache_folder: "/home/ktinh/fednlp_models"


model_args:
 model_type: "roberta" # Options: "bert", "distilbert"
 model_class: "transformer"
 model: "roberta-base"
 do_lower_case: true
 formulation: "classification"


environment_args:
 bootstrap: "config/bootstrap.sh"


train_args:
 federated_optimizer: "FedAvg"
 client_id_list: "[1, 2, 3, 4, 5]"  # Specific client IDs
 client_num_in_total: 5           # Number of clients participating
 client_num_per_round: 2          # Number of clients per communication round
 comm_round: 10                   # Increased rounds for more robust training
 epochs: 3                        # Increase local epochs for better model performance
 batch_size: 16
 eval_batch_size: 8
 max_seq_length: 128
 fp16: false
 output_dir: "/home/ktinh/fednlp_output"
 client_optimizer: "AdamW"
 server_optimizer: "sgd"
 server_momentum: 0.9
 server_lr: 0.1
 learning_rate: 0.001
 weight_decay: 0.001
 gradient_accumulation_steps: 1
 clip_grad_norm: true
 fedprox_mu: 1
 evaluate_during_training: false
 evaluate_during_training_steps: 10
 freeze_layers: ""
 is_debug_mode: true
 momentum: 0.9
 max_grad_norm: 1
 ci: 0


validation_args:
 frequency_of_the_test: 5


device_args:
 worker_num: 5                    # Server + 5 Clients
 using_gpu: false                 # Use `true` if GPUs are available


comm_args:
 backend: "MQTT_S3"                  # Changed backend to gRPC for cross-silo setup
 mqtt_config_path: "config/mqtt_config.yaml"
 s3_config_path: "config/s3_config.yaml"


tracking_args:
 enable_wandb: false
 wandb_entity: "fedml-team"
 wandb_only_server: true
 wandb_key: "404bae40078b1624faed523c4e9d25b40590e537"
 wandb_project: "fednlp"
 wandb_name: "fednlp_torch_fedavg_20news"
